{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3df8224b",
   "metadata": {},
   "source": [
    "<!-- LTeX: language=fr -->\n",
    "\n",
    "Mod√®les de langues √† n-grammes‚ÄØ: corrections\n",
    "============================================\n",
    "\n",
    "**Lo√Øc Grobol** [<lgrobol@parisnanterre.fr>](mailto:lgrobol@parisnanterre.fr)\n",
    "\n",
    "2022-09-28"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "785dec56",
   "metadata": {},
   "source": [
    "## ‚úÇÔ∏è Tokenization ‚úÇÔ∏è\n",
    "\n",
    "1\\. √âcrire une fonction `crude_tokenizer` qui prend comme argument une chaine de caract√®res et\n",
    "    renvoie la liste des mots de cette cha√Æne en s√©parant sur les espaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d38c6500",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crude_tokenizer(s):\n",
    "    return s.split()\n",
    "\n",
    "assert crude_tokenizer(\"Je reconnais l'existence du kiwi-fruit.\") == [\n",
    "    'Je', 'reconnais', \"l'existence\", 'du', 'kiwi-fruit.'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "079ed7da",
   "metadata": {},
   "source": [
    "2\\. Modifier la fonction `crude_tokenizer` pour qu'elle s√©pare aussi suivant les caract√®res\n",
    "   non alphanum√©riques. **Indice** √ßa peut √™tre utile de revoir [la doc sur les expressions\n",
    "   r√©guli√®res](https://docs.python.org/3/library/re.html) ou de relire [un tuto √† ce\n",
    "   sujet](https://realpython.com/regex-python/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfad560e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def crude_tokenizer(s):\n",
    "    return [w for w in re.split(r\"\\s|\\W\", s.strip()) if w]\n",
    "\n",
    "assert crude_tokenizer(\"Je reconnais l'existence du kiwi-fruit.\") == [\n",
    "    'Je', 'reconnais', 'l', 'existence', 'du', 'kiwi', 'fruit'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9302eb0f",
   "metadata": {},
   "source": [
    "3\\. On aimerait maintenant garder les apostrophes √† la fin du mot qui les pr√©c√®de, ainsi que les\n",
    "mots compos√©s ensemble."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "465b8900",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Je', 'reconnais', \"l'\", 'existence', 'du', 'kiwi-fruit']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re  # Si jamais on a pas ex√©cut√© la cellule pr√©c√©dente\n",
    "def crude_tokenizer(s):\n",
    "    return re.findall(r\"\\b\\w+?\\b(?:'|(?:-\\w+?\\b)*)?\", s)\n",
    "\n",
    "assert crude_tokenizer(\"Je reconnais l'existence du kiwi-fruit.\") == [\n",
    "    'Je', 'reconnais', \"l'\", 'existence', 'du', 'kiwi-fruit'\n",
    "]\n",
    "\n",
    "\n",
    "crude_tokenizer(\"Je reconnais l'existence du kiwi-fruit.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f8f559d",
   "metadata": {},
   "source": [
    "4\\. √âcrire une fonction `crude_tokenizer_and_normalizer` qui en plus de tokenizer comme pr√©c√©demment\n",
    "met tous les mots en minuscules\n",
    "\n",
    "On peut √©videmment copier-coller le code au-dessus, mais on peut aussi r√©utiliser ce qu'on a d√©j√† d√©fini‚ÄØ:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccf8327f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crude_tokenizer_and_normalizer(s):\n",
    "    return crude_tokenizer(s.lower())\n",
    "\n",
    "asser = crude_tokenizer_and_normalizer(\"Je reconnais l'existence du kiwi-fruit.\") == [\n",
    "    'je', 'reconnais', \"l'\", 'existence', 'du', 'kiwi-fruit'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aae2b492",
   "metadata": {},
   "source": [
    "## üíú Extraire les bigrammes üíú\n",
    "\n",
    "√âcrire une fonction `extract_bigrams` qui prend en entr√©e une liste de mots et renvoie la liste des bigrammes correspondants sous forme de couples de mots."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd070d46",
   "metadata": {},
   "source": [
    "Version directe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "968cf161",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_bigrams(words):\n",
    "    res = []\n",
    "    for i in range(len(words)-1):\n",
    "        res.append((words[i], words[i+1]))\n",
    "    return res\n",
    "\n",
    "assert extract_bigrams(['je', 'reconnais', \"l'\", 'existence', 'du', 'kiwi-fruit']) == [\n",
    "    ('je', 'reconnais'),\n",
    "     ('reconnais', \"l'\"),\n",
    "     (\"l'\", 'existence'),\n",
    "     ('existence', 'du'),\n",
    "     ('du', 'kiwi-fruit')\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a491d042",
   "metadata": {},
   "source": [
    "Version artistique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5cec528",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_bigrams(words):\n",
    "    return list(zip(words[:-1], words[1:]))\n",
    "\n",
    "assert extract_bigrams(['je', 'reconnais', \"l'\", 'existence', 'du', 'kiwi-fruit']) == [\n",
    "    ('je', 'reconnais'),\n",
    "     ('reconnais', \"l'\"),\n",
    "     (\"l'\", 'existence'),\n",
    "     ('existence', 'du'),\n",
    "     ('du', 'kiwi-fruit')\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58791ee1",
   "metadata": {},
   "source": [
    "Si vous trouvez √ßa obscur essayez le code ci-dessous, et allez voir ce qu'il donne [sur Python Tutor](https://pythontutor.com/render.html#code=tokenized%20%3D%20%5B'Je',%20'reconnais',%20%22l'%22,%20'existence',%20'du',%20'kiwi-fruit'%5D%0A%0Afirst_words%20%3D%20tokenized%5B%3A-1%5D%0Asecond_words%20%3D%20tokenized%5B1%3A%5D%0A%0Afor%20t%20in%20zip%28first_words,%20second_words%29%3A%0A%20%20%20%20print%28t%29&cumulative=false&curInstr=10&heapPrimitives=nevernest&mode=display&origin=opt-frontend.js&py=3&rawInputLstJSON=%5B%5D&textReferences=false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f6171c8",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "tokenized = ['je', 'reconnais', \"l'\", 'existence', 'du', 'kiwi-fruit']\n",
    "\n",
    "first_words = tokenized[:-1]\n",
    "second_words = tokenized[1:]\n",
    "\n",
    "print(first_words)\n",
    "print(second_words)\n",
    "\n",
    "for t in zip(first_words, second_words):\n",
    "    print(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7062d66",
   "metadata": {},
   "source": [
    "## üî¢ Compter üî¢"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29aa0d87",
   "metadata": {},
   "source": [
    "√âcrire une fonction `read_corpus` qui prend en argument un chemin vers un fichier texte, l'ouvre, le\n",
    "tokenize et y compte les unigrammes et les bigrammes en renvoyant deux `Counter` associant\n",
    "respectivement √† chaque mot et √† chaque bigramme leurs nombres d'occurrences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "02434e73",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'crude_tokenizer_and_normalizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [4], line 15\u001b[0m\n\u001b[0;32m     10\u001b[0m             bigrams\u001b[38;5;241m.\u001b[39mupdate(extract_bigrams(words))\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m unigrams, bigrams\n\u001b[1;32m---> 15\u001b[0m unigram_counts, bigram_counts \u001b[38;5;241m=\u001b[39m \u001b[43mread_corpus\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m./zola_ventre-de-paris.txt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m unigram_counts\u001b[38;5;241m.\u001b[39mmost_common(\u001b[38;5;241m4\u001b[39m) \u001b[38;5;241m==\u001b[39m [(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mde\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;241m5292\u001b[39m), (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mla\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;241m3565\u001b[39m), (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mles\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;241m2746\u001b[39m), (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mil\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;241m2443\u001b[39m)]\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m bigram_counts\u001b[38;5;241m.\u001b[39mmost_common(\u001b[38;5;241m4\u001b[39m) \u001b[38;5;241m==\u001b[39m [\n\u001b[0;32m     19\u001b[0m     ((\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mde\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mla\u001b[39m\u001b[38;5;124m'\u001b[39m), \u001b[38;5;241m754\u001b[39m),\n\u001b[0;32m     20\u001b[0m      ((\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mqu\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mil\u001b[39m\u001b[38;5;124m'\u001b[39m), \u001b[38;5;241m424\u001b[39m),\n\u001b[0;32m     21\u001b[0m      ((\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m√†\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mla\u001b[39m\u001b[38;5;124m'\u001b[39m), \u001b[38;5;241m336\u001b[39m),\n\u001b[0;32m     22\u001b[0m      ((\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124md\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mune\u001b[39m\u001b[38;5;124m'\u001b[39m), \u001b[38;5;241m321\u001b[39m)\n\u001b[0;32m     23\u001b[0m ]\n",
      "Cell \u001b[1;32mIn [4], line 8\u001b[0m, in \u001b[0;36mread_corpus\u001b[1;34m(file_path)\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(file_path) \u001b[38;5;28;01mas\u001b[39;00m in_stream:\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m in_stream:\n\u001b[1;32m----> 8\u001b[0m         words \u001b[38;5;241m=\u001b[39m \u001b[43mcrude_tokenizer_and_normalizer\u001b[49m(line\u001b[38;5;241m.\u001b[39mstrip())\n\u001b[0;32m      9\u001b[0m         unigrams\u001b[38;5;241m.\u001b[39mupdate(words)\n\u001b[0;32m     10\u001b[0m         bigrams\u001b[38;5;241m.\u001b[39mupdate(extract_bigrams(words))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'crude_tokenizer_and_normalizer' is not defined"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "    \n",
    "def read_corpus(file_path):\n",
    "    unigrams = Counter()\n",
    "    bigrams = Counter()\n",
    "    with open(file_path) as in_stream:\n",
    "        for line in in_stream:\n",
    "            words = crude_tokenizer_and_normalizer(line.strip())\n",
    "            unigrams.update(words)\n",
    "            bigrams.update(extract_bigrams(words))\n",
    "    \n",
    "    return unigrams, bigrams\n",
    "\n",
    "\n",
    "unigram_counts, bigram_counts = read_corpus(\"./zola_ventre-de-paris.txt\")\n",
    "\n",
    "# assert unigram_counts.most_common(4) == [('de', 5292), ('la', 3565), ('les', 2746), ('il', 2443)]\n",
    "# assert bigram_counts.most_common(4) == [\n",
    "#     (('de', 'la'), 754),\n",
    "#      ((\"qu'\", 'il'), 424),\n",
    "#      (('√†', 'la'), 336),\n",
    "#      ((\"d'\", 'une'), 321)\n",
    "# ]\n",
    "\n",
    "unigram_counts.most_common(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb02d42e",
   "metadata": {},
   "source": [
    "## ü§ì Estimer les probas ü§ì"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c30a3d7",
   "metadata": {},
   "source": [
    "On va ensuite estimer les probabilit√©s de transition, c'est-√†-dire la probabilit√© de g√©n√©rer un\n",
    "certain mot $w_1$ sachant que le mot pr√©c√©dent est $w_0$. On le fait en utilisant la formule du\n",
    "maximum de vraisemblance‚ÄØ:\n",
    "\n",
    "\\begin{equation}\n",
    "   P(w_1|w_0) := P\\!\\left([w_0, w_1]~|~[w_0, *]\\right) = \\frac{\\text{nombre d'occurrences du bigramme $w_0 w_1$}}{\\text{nombre d'occurrences de l'unigramme $w_0$}}\n",
    "\\end{equation}\n",
    "\n",
    "Pour que ce soit plus agr√©able √† sampler on va utiliser un dictionnaire de dictionnaires‚ÄØ:\n",
    "`probs[v][w]` stockera $P(w|v)$.\n",
    "\n",
    "√Ä vous de jouer‚ÄØ: √©crire une fonction `get_probs`, qui prend en entr√©e la les compteurs de bigrammes\n",
    "et d'unigrammes et renvoie le dictionnaire `probs`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d33bce59",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_probs(unigram_counts, bigram_counts):\n",
    "    probs = dict()\n",
    "    for (v, w), c in bigram_counts.items():\n",
    "        if v not in probs:\n",
    "            # Si on a pas encore rencontr√© de bigrammes commen√ßant par `v`, il faut\n",
    "            # commencer par cr√©er `probs[v]`\n",
    "            probs[v] = dict()\n",
    "        probs[v][w] = c/unigram_counts[v]\n",
    "    return probs\n",
    "\n",
    "probs = get_probs(unigram_counts, bigram_counts)\n",
    "assert probs[\"je\"][\"d√©jeune\"] == 0.002232142857142857"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d136fee4",
   "metadata": {},
   "source": [
    "Avec `collections.defaultdict`‚ÄØ:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0a26cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def get_probs(unigram_counts, bigram_counts):\n",
    "    # Un dictionnaire de dictionnaires cr√©√©s automatiquement √† l'acc√®s,\n",
    "    # √áa √©vite de faire un test et √ßa rend souvent le code plus lisible\n",
    "    probs = defaultdict(dict)\n",
    "    for (v, w), c in bigram_counts.items():\n",
    "        probs[v][w] = c/unigram_counts[v]\n",
    "\n",
    "    # Pour ne pas masquer des erreurs pendant le sampling, on en refait un dict normal\n",
    "    return dict(probs)\n",
    "\n",
    "probs = get_probs(unigram_counts, bigram_counts)\n",
    "assert probs[\"je\"][\"d√©jeune\"] == 0.002232142857142857"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "045bba6a",
   "metadata": {},
   "source": [
    "## ü§î G√©n√©rer ü§î\n",
    "\n",
    "Pour l'instant on ne va pas se pr√©occuper de sauvegarder le mod√®le on va l'utiliser directement pour\n",
    "sampler. Le principe est simple‚ÄØ: on choisit le premier mot, puis on choisit le deuxi√®me mot en\n",
    "prenant en compte celui qu'on vient de g√©n√©rer (le premier donc si vous suivez) et ainsi de suite."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5daf19b5",
   "metadata": {},
   "source": [
    "**Questions**\n",
    "\n",
    "- Comment on choisit le premier mot‚ÄØ?\n",
    "- Et quand est-ce qu'on d√©cide de s'arr√™ter‚ÄØ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c59f29c",
   "metadata": {},
   "source": [
    "Jurafsky et Martin nous disent\n",
    "\n",
    ">  We‚Äôll first need to augment each sentence with a special symbol `<s>` at the beginning of the\n",
    "> sentence, to give us the bigram context of the first word. We‚Äôll also need a special end-symbol.\n",
    "> `</s>`\n",
    "\n",
    "Heureusement on a un fichier bien fait‚ÄØ: il y a une seule phrase par ligne."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "720b0911",
   "metadata": {},
   "source": [
    "1\\. Modifier `read_corpus` pour ajouter ajouter √† la vol√©e `<s>` au d√©but de chaque ligne et `</s>` √† la fin de chaque ligne."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66a54179",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_corpus(file_path):\n",
    "    unigrams = Counter()\n",
    "    bigrams = Counter()\n",
    "    with open(file_path) as in_stream:\n",
    "        for line in in_stream:\n",
    "            words = crude_tokenizer_and_normalizer(line.strip())\n",
    "            words.insert(0, \"<s>\")\n",
    "            words.append(\"</s>\")\n",
    "            unigrams.update(words)\n",
    "            bigrams.update(extract_bigrams(words))\n",
    "    \n",
    "    return unigrams, bigrams\n",
    "\n",
    "\n",
    "unigram_counts, bigram_counts = read_corpus(\"data/zola_ventre-de-paris.txt\")\n",
    "\n",
    "assert unigram_counts.most_common(4) == [('<s>', 8945), ('</s>', 8945), ('de', 5292), ('la', 3565)]\n",
    "assert bigram_counts.most_common(4) == [\n",
    "    (('<s>', '</s>'), 1811),\n",
    "    (('<s>', 'il'), 775),\n",
    "    (('de', 'la'), 754),\n",
    "    (('<s>', 'elle'), 576)\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "701a07c4",
   "metadata": {},
   "source": [
    "Il y a encore un petit probl√®me"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6568d17",
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_counts.most_common(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b584cf0",
   "metadata": {},
   "source": [
    "ü§î"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a65c5bc3",
   "metadata": {},
   "source": [
    "On a compt√© les lignes vides üò§. √áa ne posait pas de probl√®me jusque-l√† puisque √ßa n'ajoutait rien\n",
    "aux compteurs de n-grammes, mais maintenant √ßa nous fait des `[\"<s>\", \"</s>\"]`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bce3674",
   "metadata": {},
   "source": [
    "2\\. Modifier `read_corpus` pour ignorer les lignes vides"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d74ad21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_corpus(file_path):\n",
    "    unigrams = Counter()\n",
    "    bigrams = Counter()\n",
    "    with open(file_path) as in_stream:\n",
    "        for line in in_stream:\n",
    "            if line.isspace():\n",
    "                continue\n",
    "            words = crude_tokenizer_and_normalizer(line.strip())\n",
    "            words.insert(0, \"<s>\")\n",
    "            words.append(\"</s>\")\n",
    "            unigrams.update(words)\n",
    "            bigrams.update(extract_bigrams(words))\n",
    "    \n",
    "    return unigrams, bigrams\n",
    "\n",
    "\n",
    "unigram_counts, bigram_counts = read_corpus(\"data/zola_ventre-de-paris.txt\")\n",
    "\n",
    "assert unigram_counts.most_common(4) == [('<s>', 7145), ('</s>', 7145), ('de', 5292), ('la', 3565)]\n",
    "assert bigram_counts.most_common(4) == [\n",
    "    (('<s>', 'il'), 775),\n",
    "    (('de', 'la'), 754),\n",
    "    (('<s>', 'elle'), 576),\n",
    "    ((\"qu'\", 'il'), 424)\n",
    "]\n",
    "\n",
    "probs = get_probs(unigram_counts, bigram_counts)\n",
    "assert probs[\"<s>\"][\"le\"] == 0.0298110566829951"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f76e33b",
   "metadata": {},
   "source": [
    "## üòå G√©n√©rer pour de vrai üòå\n",
    "\n",
    "**Bon c'est bon maintenant‚ÄØ?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5511860c",
   "metadata": {},
   "source": [
    "√Ä peu pr√®s. On va pouvoir sampler."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73d15985",
   "metadata": {},
   "source": [
    "Pour √ßa on va piocher dans le module [`random`](https://docs.python.org/3/library/random.html) de la\n",
    "biblioth√®que standard, et en particulier la fonction\n",
    "[`random.choices`](https://docs.python.org/3/library/random.html#random.choices) qui permet de tirer\n",
    "au sort dans une population finie en pr√©cisant les probabilit√©s de chacun de √©l√©ments. Le poids\n",
    "n'ont en principe pas besoin d'√™tre normalis√©s (mais ils le seront ici, √©videmment)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a9cd3a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b50e803c",
   "metadata": {},
   "source": [
    "Voici par exemple comment choisir un mot qui suivrait ¬´‚ÄØje‚ÄØ¬ª‚ÄØ:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85b20b82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Les candidats mots qui peuvent suivre ¬´‚ÄØje‚ÄØ¬ª\n",
    "candidates = list(probs[\"je\"].keys())\n",
    "# Leurs poids, ce sont les probabilit√©s qu'on a d√©j√† calcul√©\n",
    "weights = [probs[\"je\"][c] for c in candidates] \n",
    "random.choices(candidates, weights, k=1)[0]  # Attention `choices` renvoit une liste"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73449109",
   "metadata": {},
   "source": [
    "√âcrire une fonction `sample` qui prend en argument les probabilit√©s de bigrammes (sous la forme d'un dictionnaire de dictionnaires comme notre `prob`) et g√©n√®re une phrase en partant de `<s>` et en ajoutant des mots it√©rativement, s'arr√™tant quand `</s>` a √©t√© choisi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8a4d9fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(bigram_probs):\n",
    "    sent = [\"<s>\"]\n",
    "    while sent[-1] != \"</s>\":\n",
    "        candidates = list(probs[sent[-1]].keys())\n",
    "        weights = [probs[sent[-1]][c] for c in candidates]\n",
    "        sent.append(random.choices(candidates, weights)[0])\n",
    "    return sent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7c470cc",
   "metadata": {},
   "source": [
    "Pas de assert ici comme on a de l'al√©atoire, mais la cellule suivante permet de tester si √ßa marche"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54b069dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sample(probs))\n",
    "print(\" \".join(sample(probs)[1:-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d796bc6",
   "metadata": {},
   "source": [
    "C'est rigolo, hein‚ÄØ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "270d204c",
   "metadata": {},
   "source": [
    "Qu'est-ce que vous pensez des textes qu'on g√©n√®re‚ÄØ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1058e60",
   "metadata": {},
   "source": [
    "## üßê Aller plus loin üßê"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3301c603",
   "metadata": {},
   "source": [
    "En vous inspirant de ce qui a √©t√© fait, coder un g√©n√©rateur de phrases √† partir de trigrammes,\n",
    "t√©tragrammes (4), puis de n-grammes arbitraires.\n"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,md",
   "split_at_heading": true
  },
  "kernelspec": {
   "display_name": "Python 3.10.7 64-bit (microsoft store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "ca9a115054207d2797d1b78f16e1bfbf4323fc9a87b54d9e455f876acde93457"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
